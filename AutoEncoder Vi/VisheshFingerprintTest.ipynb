{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YElBzHmaikaS"
      },
      "source": [
        "Metric  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWu5HjhEij4L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_rmse(img1, img2):\n",
        "    return np.sqrt(np.mean((img1 - img2) ** 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOfd3rbVbE0L"
      },
      "source": [
        "Unarchive the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R38BX9EMZHV6"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "# Upload zip manually or via Google Drive\n",
        "!unzip /content/archive.zip -d /content/fingerprints/\n",
        "\n",
        "# Preview a few image paths\n",
        "import glob\n",
        "image_paths = glob.glob('/content/fingerprints/**/*.tif', recursive=True)  # Adjust extension if needed\n",
        "print(\"Total images:\", len(image_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amVgAtYEbKp4"
      },
      "source": [
        "Preprocess images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fox5a96zbKPV"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def load_images(paths, size=(224, 224)):\n",
        "    images = []\n",
        "    for path in paths:\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, size)\n",
        "            img = img.astype('float32') / 255.0\n",
        "            images.append(img)\n",
        "    images = np.array(images)\n",
        "    images = np.expand_dims(images, axis=-1)  # Shape: (N, 224, 224, 1)\n",
        "    return images\n",
        "\n",
        "clean_images = load_images(image_paths)\n",
        "print(\"Clean image shape:\", clean_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK-cBVBDbdVb"
      },
      "source": [
        "Add noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-GCfy8DbfIy"
      },
      "outputs": [],
      "source": [
        "def add_noise(images, noise_factor=0.3):\n",
        "    noisy = images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)\n",
        "    return np.clip(noisy, 0., 1.)\n",
        "\n",
        "noisy_images = add_noise(clean_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92wbwYtNbiCv"
      },
      "source": [
        "TT Split  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRf76j0YbhFD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, valid_X, train_Y, valid_Y = train_test_split(noisy_images, clean_images, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKlfFF-UblkT"
      },
      "source": [
        "Convolution Auto Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qorkx_2wbo_Q"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "\n",
        "input_img = Input(shape=(224, 224, 1))\n",
        "\n",
        "# Encoder\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# Decoder\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328U0ESXbuAH"
      },
      "source": [
        "Main training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFyuby02bxR_",
        "outputId": "af8d6b82-0d70-42ba-ff7c-8e4020a8b4a8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ§  Training from epoch 1 to 100...\n",
            "\n",
            "Epoch 1/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 6s/step - loss: 0.5586 - val_loss: 0.5456\n",
            "Epoch 2/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 6s/step - loss: 0.5665 - val_loss: 0.5403\n",
            "Epoch 3/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7s/step - loss: 0.5666 - val_loss: 0.5381\n",
            "Epoch 4/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 6s/step - loss: 0.5593 - val_loss: 0.5362\n",
            "Epoch 5/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 6s/step - loss: 0.5536 - val_loss: 0.5354\n",
            "Epoch 6/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 6s/step - loss: 0.5642 - val_loss: 0.5351\n",
            "Epoch 7/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5723 - val_loss: 0.5351\n",
            "Epoch 8/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - loss: 0.5565 - val_loss: 0.5347\n",
            "Epoch 9/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6s/step - loss: 0.5384 - val_loss: 0.5347\n",
            "Epoch 10/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - loss: 0.5439 - val_loss: 0.5338\n",
            "Epoch 11/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 7s/step - loss: 0.5676 - val_loss: 0.5336\n",
            "Epoch 12/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6s/step - loss: 0.5283 - val_loss: 0.5329\n",
            "Epoch 13/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5376 - val_loss: 0.5329\n",
            "Epoch 14/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5450 - val_loss: 0.5323\n",
            "Epoch 15/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5600 - val_loss: 0.5318\n",
            "Epoch 16/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6s/step - loss: 0.5458 - val_loss: 0.5312\n",
            "Epoch 17/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6s/step - loss: 0.5380 - val_loss: 0.5322\n",
            "Epoch 18/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5582 - val_loss: 0.5325\n",
            "Epoch 19/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6s/step - loss: 0.5581 - val_loss: 0.5304\n",
            "Epoch 20/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 7s/step - loss: 0.5546 - val_loss: 0.5300\n",
            "Epoch 21/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 6s/step - loss: 0.5524 - val_loss: 0.5296\n",
            "Epoch 22/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 6s/step - loss: 0.5474 - val_loss: 0.5293\n",
            "Epoch 23/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6s/step - loss: 0.5456 - val_loss: 0.5290\n",
            "Epoch 24/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6s/step - loss: 0.5449 - val_loss: 0.5289\n",
            "Epoch 25/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5425 - val_loss: 0.5283\n",
            "Epoch 26/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 6s/step - loss: 0.5415 - val_loss: 0.5280\n",
            "Epoch 27/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - loss: 0.5480 - val_loss: 0.5273\n",
            "Epoch 28/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 7s/step - loss: 0.5598 - val_loss: 0.5269\n",
            "Epoch 29/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 6s/step - loss: 0.5532 - val_loss: 0.5275\n",
            "Epoch 30/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5502 - val_loss: 0.5274\n",
            "Epoch 31/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5550 - val_loss: 0.5263\n",
            "Epoch 32/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - loss: 0.5580 - val_loss: 0.5254\n",
            "Epoch 33/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6s/step - loss: 0.5579 - val_loss: 0.5248\n",
            "Epoch 34/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5547 - val_loss: 0.5243\n",
            "Epoch 35/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5546 - val_loss: 0.5244\n",
            "Epoch 36/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 7s/step - loss: 0.5461 - val_loss: 0.5248\n",
            "Epoch 37/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6s/step - loss: 0.5403 - val_loss: 0.5238\n",
            "Epoch 38/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 6s/step - loss: 0.5618 - val_loss: 0.5232\n",
            "Epoch 39/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - loss: 0.5434 - val_loss: 0.5228\n",
            "Epoch 40/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6s/step - loss: 0.5416 - val_loss: 0.5224\n",
            "Epoch 41/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5503 - val_loss: 0.5221\n",
            "Epoch 42/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5458 - val_loss: 0.5219\n",
            "Epoch 43/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6s/step - loss: 0.5392 - val_loss: 0.5217\n",
            "Epoch 44/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5408 - val_loss: 0.5215\n",
            "Epoch 45/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.5473 - val_loss: 0.5214\n",
            "Epoch 46/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7s/step - loss: 0.5457 - val_loss: 0.5210\n",
            "Epoch 47/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 6s/step - loss: 0.5386 - val_loss: 0.5207\n",
            "Epoch 48/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - loss: 0.5557 - val_loss: 0.5224\n",
            "Epoch 49/100\n",
            "\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m11s\u001b[0m 6s/step - loss: 0.5629"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "# Loss trackers\n",
        "full_train_loss = []\n",
        "full_val_loss = []\n",
        "\n",
        "# Total training setup\n",
        "total_epochs = 1000\n",
        "save_interval = 200\n",
        "epochs_per_phase = 100  # Can be tuned\n",
        "\n",
        "# Training in chunks\n",
        "for start_epoch in range(0, total_epochs, epochs_per_phase):\n",
        "    end_epoch = min(start_epoch + epochs_per_phase, total_epochs)\n",
        "\n",
        "    print(f\"\\nðŸ§  Training from epoch {start_epoch + 1} to {end_epoch}...\\n\")\n",
        "\n",
        "    history = autoencoder.fit(\n",
        "        train_X, train_Y,\n",
        "        initial_epoch=start_epoch,\n",
        "        epochs=end_epoch,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        validation_data=(valid_X, valid_Y)\n",
        "    )\n",
        "\n",
        "    # Append losses\n",
        "    full_train_loss += history.history['loss']\n",
        "    full_val_loss += history.history['val_loss']\n",
        "\n",
        "    # Save checkpoint if required\n",
        "    if (end_epoch % save_interval == 0) or (end_epoch == total_epochs):\n",
        "        autoencoder.save(f'checkpoints/autoencoder_epoch_{end_epoch}.h5')\n",
        "        print(f\"âœ… Model checkpoint saved at epoch {end_epoch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UC5gfLkhKtK"
      },
      "source": [
        "Plotting curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHUsXToFhMlK"
      },
      "outputs": [],
      "source": [
        "# Plot after training all 1000 epochs\n",
        "epochs = range(1, len(full_train_loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, full_train_loss, label='Training Loss', color='blue')\n",
        "plt.plot(epochs, full_val_loss, label='Validation Loss', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (1 to 1000 epochs)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"loss_graph.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjbvuWCodAvp"
      },
      "source": [
        "Evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mza6H9yKlYLI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Select 5 random images from dataset\n",
        "sample_indices = random.sample(range(len(clean_images)), 5)\n",
        "original = clean_images[sample_indices]\n",
        "noisy = add_noise(original)\n",
        "\n",
        "model = load_model(f'checkpoints/autoencoder_epoch_1000.h5')\n",
        "\n",
        "# Predict using each model\n",
        "predictions = model.predict(noisy)\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "\n",
        "for i in range(5):\n",
        "    rmse_before = compute_rmse(original[i], noisy[i])\n",
        "    rmse_after = compute_rmse(original[i], predictions[i])\n",
        "\n",
        "    # Row 1: Clean\n",
        "    plt.subplot(3, 5, i + 1)\n",
        "    plt.imshow(original[i].squeeze(), cmap='gray')\n",
        "    plt.title(sample_indices[i])\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Row 2: Noisy\n",
        "    plt.subplot(3, 5, i + 6)\n",
        "    plt.imshow(noisy[i].squeeze(), cmap='gray')\n",
        "    plt.title(f\"RMSE (Original vs Noisy)     : {rmse_before:.4f}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Row 3+: Denoised by each checkpoint\n",
        "    plt.subplot(3, 5, i + 11)\n",
        "    plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
        "    plt.title(f\"RMSE (Original vs Denoised)  : {rmse_after:.4f}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijAEC48dOFMc"
      },
      "source": [
        "Sreeja's test script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHBd6Bc2OEfX"
      },
      "outputs": [],
      "source": [
        "# # STEP 1: Upload & Unzip Dataset\n",
        "# # -----------------------------------------------\n",
        "# import zipfile\n",
        "# import glob\n",
        "\n",
        "# with zipfile.ZipFile(\"/content/archive.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"/content/fingerprints\")\n",
        "\n",
        "# print(\"âœ… Dataset extracted.\")\n",
        "# all_images = glob.glob(\"/content/fingerprints/**/*.tif\", recursive=True)\n",
        "# print(f\"Total images found: {len(all_images)}\")\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 2: Preprocessing\n",
        "# # -----------------------------------------------\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "\n",
        "# def crop_image_from_gray(img):\n",
        "#     if len(img.shape) == 3:\n",
        "#         gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "#     else:\n",
        "#         gray = img\n",
        "#     mask = gray > 10\n",
        "#     if mask.any():\n",
        "#         img = img[np.ix_(mask.any(1), mask.any(0))]\n",
        "#     return img\n",
        "\n",
        "# def process_image(path):\n",
        "#     img = cv2.imread(path)\n",
        "#     img = crop_image_from_gray(img)\n",
        "#     img = cv2.resize(img, (128, 128))\n",
        "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "#     img = img / 255.0\n",
        "#     img = np.expand_dims(img, axis=-1)\n",
        "#     return img\n",
        "\n",
        "# processed_images = [process_image(p) for p in all_images]\n",
        "# processed_images = np.array(processed_images)\n",
        "# print(f\"âœ… Processed images shape: {processed_images.shape}\")\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 3: Noise Injection\n",
        "# # -----------------------------------------------\n",
        "# def add_noise(image, stage):\n",
        "#     noise_factors = {1: 0.1, 2: 0.2, 3: 0.3, 4: 0.3}\n",
        "#     noise = np.random.normal(0, 1, image.shape)\n",
        "#     noisy_image = image + noise_factors[stage] * noise\n",
        "\n",
        "#     if stage == 4:\n",
        "#         noisy_image = cv2.GaussianBlur(noisy_image.squeeze(), (5, 5), 0)\n",
        "#         noisy_image = np.expand_dims(noisy_image, axis=-1)\n",
        "\n",
        "#     noisy_image = np.clip(noisy_image, 0, 1)\n",
        "#     return noisy_image\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 4: Train-Test Split\n",
        "# # -----------------------------------------------\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train_clean, test_clean = train_test_split(processed_images, test_size=0.2, random_state=42)\n",
        "# train_noisy = np.array([add_noise(img, stage=3) for img in train_clean])\n",
        "# test_noisy = np.array([add_noise(img, stage=3) for img in test_clean])\n",
        "\n",
        "# print(\"Train set:\", train_clean.shape)\n",
        "# print(\"Test set:\", test_clean.shape)\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 5: Dataset Preparation for TensorFlow\n",
        "# # -----------------------------------------------\n",
        "# import tensorflow as tf\n",
        "\n",
        "# BATCH_SIZE = 8\n",
        "# BUFFER_SIZE = len(train_noisy)\n",
        "\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((train_noisy, train_clean))\n",
        "# train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 6: Build U-Net Generator\n",
        "# # -----------------------------------------------\n",
        "# from tensorflow.keras import layers, Model\n",
        "\n",
        "# def unet_generator():\n",
        "#     inputs = layers.Input(shape=(128, 128, 1))\n",
        "\n",
        "#     e1 = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
        "#     e1 = layers.LeakyReLU()(e1)\n",
        "\n",
        "#     e2 = layers.Conv2D(128, 4, strides=2, padding='same')(e1)\n",
        "#     e2 = layers.BatchNormalization()(e2)\n",
        "#     e2 = layers.LeakyReLU()(e2)\n",
        "\n",
        "#     e3 = layers.Conv2D(256, 4, strides=2, padding='same')(e2)\n",
        "#     e3 = layers.BatchNormalization()(e3)\n",
        "#     e3 = layers.LeakyReLU()(e3)\n",
        "\n",
        "#     b = layers.Conv2D(512, 4, strides=2, padding='same')(e3)\n",
        "#     b = layers.BatchNormalization()(b)\n",
        "#     b = layers.ReLU()(b)\n",
        "\n",
        "#     d1 = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(b)\n",
        "#     d1 = layers.BatchNormalization()(d1)\n",
        "#     d1 = layers.Concatenate()([d1, e3])\n",
        "#     d1 = layers.ReLU()(d1)\n",
        "\n",
        "#     d2 = layers.Conv2DTranspose(128, 4, strides=2, padding='same')(d1)\n",
        "#     d2 = layers.BatchNormalization()(d2)\n",
        "#     d2 = layers.Concatenate()([d2, e2])\n",
        "#     d2 = layers.ReLU()(d2)\n",
        "\n",
        "#     d3 = layers.Conv2DTranspose(64, 4, strides=2, padding='same')(d2)\n",
        "#     d3 = layers.BatchNormalization()(d3)\n",
        "#     d3 = layers.Concatenate()([d3, e1])\n",
        "#     d3 = layers.ReLU()(d3)\n",
        "\n",
        "#     outputs = layers.Conv2DTranspose(1, 4, strides=2, padding='same', activation='sigmoid')(d3)\n",
        "#     return Model(inputs, outputs)\n",
        "\n",
        "# generator = unet_generator()\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 7: Build Discriminator\n",
        "# # -----------------------------------------------\n",
        "# def build_discriminator():\n",
        "#     input_noisy = layers.Input(shape=(128, 128, 1))\n",
        "#     input_clean = layers.Input(shape=(128, 128, 1))\n",
        "\n",
        "#     combined = layers.Concatenate()([input_noisy, input_clean])\n",
        "\n",
        "#     d = layers.Conv2D(64, 4, strides=2, padding='same')(combined)\n",
        "#     d = layers.LeakyReLU()(d)\n",
        "\n",
        "#     d = layers.Conv2D(128, 4, strides=2, padding='same')(d)\n",
        "#     d = layers.BatchNormalization()(d)\n",
        "#     d = layers.LeakyReLU()(d)\n",
        "\n",
        "#     d = layers.Conv2D(256, 4, strides=2, padding='same')(d)\n",
        "#     d = layers.BatchNormalization()(d)\n",
        "#     d = layers.LeakyReLU()(d)\n",
        "\n",
        "#     d = layers.Conv2D(1, 4, strides=1, padding='same')(d)\n",
        "#     output = layers.Activation('sigmoid')(d)\n",
        "\n",
        "#     return Model([input_noisy, input_clean], output)\n",
        "\n",
        "# discriminator = build_discriminator()\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 8: Loss Functions and Optimizers\n",
        "# # -----------------------------------------------\n",
        "# bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "# l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "# def generator_loss(disc_output, gen_output, target):\n",
        "#     adv_loss = bce(tf.ones_like(disc_output), disc_output)\n",
        "#     l1 = l1_loss(target, gen_output)\n",
        "#     return adv_loss + 100 * l1\n",
        "\n",
        "# def discriminator_loss(disc_real, disc_generated):\n",
        "#     real_loss = bce(tf.ones_like(disc_real), disc_real)\n",
        "#     gen_loss = bce(tf.zeros_like(disc_generated), disc_generated)\n",
        "#     return real_loss + gen_loss\n",
        "\n",
        "# generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "# discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 9: Training Step\n",
        "# # -----------------------------------------------\n",
        "# @tf.function\n",
        "# def train_step(input_noisy, target_clean):\n",
        "#     with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "#         gen_output = generator(input_noisy, training=True)\n",
        "#         disc_real = discriminator([input_noisy, target_clean], training=True)\n",
        "#         disc_generated = discriminator([input_noisy, gen_output], training=True)\n",
        "\n",
        "#         gen_loss = generator_loss(disc_generated, gen_output, target_clean)\n",
        "#         disc_loss = discriminator_loss(disc_real, disc_generated)\n",
        "\n",
        "#     gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "#     gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "#     generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
        "#     discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
        "\n",
        "#     return gen_loss, disc_loss\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 10: Full Training Loop\n",
        "# # -----------------------------------------------\n",
        "# import time\n",
        "\n",
        "# EPOCHS = 50\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     start = time.time()\n",
        "#     gen_losses = []\n",
        "#     disc_losses = []\n",
        "\n",
        "#     for input_noisy_batch, target_clean_batch in train_dataset:\n",
        "#         gen_loss, disc_loss = train_step(input_noisy_batch, target_clean_batch)\n",
        "#         gen_losses.append(gen_loss.numpy())\n",
        "#         disc_losses.append(disc_loss.numpy())\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{EPOCHS} - Gen Loss: {np.mean(gen_losses):.4f} - Disc Loss: {np.mean(disc_losses):.4f} - Time: {time.time()-start:.2f}s\")\n",
        "\n",
        "# # -----------------------------------------------\n",
        "# # STEP 11: Evaluation on Test Set\n",
        "# # -----------------------------------------------\n",
        "# restored_test = generator.predict(test_noisy)\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import random\n",
        "\n",
        "# test_idx = random.sample(range(len(test_clean)), 5)\n",
        "\n",
        "# plt.figure(figsize=(15, 5))\n",
        "\n",
        "# for i, idx in enumerate(test_idx):\n",
        "#     plt.subplot(3, 5, i+1)\n",
        "#     plt.imshow(test_clean[idx].squeeze(), cmap='gray')\n",
        "#     plt.title(\"Clean\")\n",
        "#     plt.axis('off')\n",
        "\n",
        "#     plt.subplot(3, 5, i+6)\n",
        "#     plt.imshow(test_noisy[idx].squeeze(), cmap='gray')\n",
        "#     plt.title(\"Noisy\")\n",
        "#     plt.axis('off')\n",
        "\n",
        "#     plt.subplot(3, 5, i+11)\n",
        "#     plt.imshow(restored_test[idx].squeeze(), cmap='gray')\n",
        "#     plt.title(\"Restored\")\n",
        "#     plt.axis('off')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}